import numpy as np
from tsai.all import F

from torch import Tensor
import torch
import torch.nn as nn

from typing import List, Dict, Optional


def ifnone(a, b):
    """Returns b if a is None else a"""
    return b if a is None else a


class _Flatten(nn.Module):
    def __init__(self, full=False):
        super().__init__()
        self.full = full
    
    def forward(self, x):
        return x.view(-1) if self.full else x.view(x.size(0), -1)


class Sequential(nn.Sequential):
    """Class that allows you to pass one or multiple inputs"""
    def forward(self, *x):
        for i, module in enumerate(self._modules.values()):
            x = module(*x) if isinstance(x, (list, tuple)) else module(x)
        return x

class _MLP(nn.Module):
    def __init__(self, dims, bn=False, act=None, skip=False, dropout=0., bn_final=False):
        super().__init__()
        dims_pairs = list(zip(dims[:-1], dims[1:]))
        layers = []
        for i, (dim_in, dim_out) in enumerate(dims_pairs):
            is_last = i >= (len(dims) - 2)
            if bn and (not is_last or bn_final): 
                layers.append(nn.BatchNorm1d(dim_in))
            if dropout and not is_last:
                layers.append(nn.Dropout(dropout))
            layers.append(nn.Linear(dim_in, dim_out))
            if is_last: 
                break
            layers.append(ifnone(act, nn.ReLU()))
        self.mlp = nn.Sequential(*layers)
        self.shortcut = nn.Linear(dims[0], dims[-1]) if skip else None

    def forward(self, x):
        if self.shortcut is not None: 
            return self.mlp(x) + self.shortcut(x)
        else:
            return self.mlp(x)

    
class _TabFusionEncoder(nn.Module):
    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None, 
                 res_dropout=0.1, activation='gelu', res_attention=False, n_layers=1):
        super().__init__()
        self.layers = nn.ModuleList([
            _TabFusionEncoderLayer(q_len, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, 
                                   d_ff=d_ff, res_dropout=res_dropout, 
                                   activation=activation, res_attention=res_attention) 
            for i in range(n_layers)
        ])
        self.res_attention = res_attention

    def forward(self, src, attn_mask=None, key_padding_mask: Optional[Tensor] = None):
        output = src
        scores = None
        if self.res_attention:
            for mod in self.layers: 
                output, scores = mod(output, prev=scores, attn_mask=attn_mask, 
                                    key_padding_mask=key_padding_mask)
            return output
        else:
            for mod in self.layers: 
                output = mod(output, attn_mask=attn_mask, key_padding_mask=key_padding_mask)
            return output



class _TabFusionEncoderLayer(nn.Module):
    def __init__(self, q_len, d_model, n_heads, d_k=None, d_v=None, d_ff=None, 
                 res_dropout=0.1, activation="gelu", res_attention=False):
        super().__init__()
        assert not d_model % n_heads, f"d_model ({d_model}) must be divisible by n_heads ({n_heads})"
        d_k = ifnone(d_k, d_model // n_heads)
        d_v = ifnone(d_v, d_model // n_heads)
        d_ff = ifnone(d_ff, d_model * 4)
        
        # Multi-Head attention
        self.res_attention = res_attention
        self.self_attn = _MultiheadAttention(d_model, n_heads, d_k, d_v, res_attention=res_attention)
        
        # Add & Norm
        self.dropout_attn = nn.Dropout(res_dropout)
        self.layernorm_attn = nn.LayerNorm(d_model)
        
        # Position-wise Feed-Forward
        self.ff = nn.Sequential(
            nn.Linear(d_model, d_ff), 
            self._get_activation_fn(activation), 
            nn.Linear(d_ff, d_model)
        )
        
        # Add & Norm
        self.dropout_ffn = nn.Dropout(res_dropout)
        self.layernorm_ffn = nn.LayerNorm(d_model)

    def forward(self, src, prev=None, attn_mask=None, key_padding_mask: Optional[Tensor] = None):
        # Multi-Head attention sublayer
        if self.res_attention:
            src2, attn, scores = self.self_attn(src, src, src, prev, 
                                                 key_padding_mask=key_padding_mask, 
                                                 attn_mask=attn_mask)
        else:
            src2, attn = self.self_attn(src, src, src, 
                                        key_padding_mask=key_padding_mask, 
                                        attn_mask=attn_mask)
        self.attn = attn
        
        # Add & Norm
        src = src + self.dropout_attn(src2)
        src = self.layernorm_attn(src)
        
        # Feed-forward sublayer
        src2 = self.ff(src)
        
        # Add & Norm
        src = src + self.dropout_ffn(src2)
        src = self.layernorm_ffn(src)
        
        if self.res_attention:
            return src, scores
        else:
            return src

    def _get_activation_fn(self, activation):
        if callable(activation): 
            return activation()
        elif activation.lower() == "relu": 
            return nn.ReLU()
        elif activation.lower() == "gelu": 
            return nn.GELU()
        else:
            return nn.GELU()


class _MultiheadAttention(nn.Module):
    def __init__(self, d_model: int, n_heads: int, d_k: int, d_v: int, res_attention: bool = False):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_k
        self.d_v = d_v
        
        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)
        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)
        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)
        self.W_O = nn.Linear(n_heads * d_v, d_model, bias=False)
        
        self.res_attention = res_attention
        self.sdp_attn = _ScaledDotProductAttention(self.d_k, self.res_attention)
        
    def forward(self, Q, K, V, prev=None, attn_mask=None, key_padding_mask: Optional[Tensor] = None):
        bs = Q.size(0)
        
        # Linear (+ split in multiple heads)
        q_s = self.W_Q(Q).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)
        k_s = self.W_K(K).view(bs, -1, self.n_heads, self.d_k).permute(0, 2, 3, 1)
        v_s = self.W_V(V).view(bs, -1, self.n_heads, self.d_v).transpose(1, 2)
        
        # Scaled Dot-Product Attention
        if self.res_attention:
            context, attn, scores = self.sdp_attn(q_s, k_s, v_s, prev=prev, 
                                                   key_padding_mask=key_padding_mask, 
                                                   attn_mask=attn_mask)
        else:
            context, attn = self.sdp_attn(q_s, k_s, v_s, 
                                          key_padding_mask=key_padding_mask, 
                                          attn_mask=attn_mask)
        
        # Concat
        context = context.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * self.d_v)
        
        # Linear
        output = self.W_O(context)
        
        if self.res_attention: 
            return output, attn, scores
        else: 
            return output, attn
    
################################################




class _ScaledDotProductAttention(nn.Module):
    def __init__(self, d_k: int, res_attention: bool = False): 
        super().__init__()
        self.d_k = d_k
        self.res_attention = res_attention
        
    def forward(self, q, k, v, prev=None, attn_mask=None, key_padding_mask: Optional[Tensor] = None):
        # MatMul (q, k) - similarity scores
        scores = torch.matmul(q, k)
        
        # Scale
        scores = scores / (self.d_k ** 0.5)
        
        # Attention mask (optional)
        if attn_mask is not None:
            if attn_mask.dtype == torch.bool:
                scores.masked_fill_(attn_mask, float('-inf'))
            else:
                scores += attn_mask
        
        # Key padding mask (optional)
        if key_padding_mask is not None:
            scores.masked_fill_(key_padding_mask.unsqueeze(1).unsqueeze(2), -np.inf)
        
        # SoftMax
        if prev is not None: 
            scores = scores + prev
        
        attn = F.softmax(scores, dim=-1)
        
        # MatMul (attn, v)
        context = torch.matmul(attn, v)
        
        if self.res_attention: 
            return context, attn, scores
        else: 
            return context, attn



class MultiHotEmbedding(nn.Module):
    """
    Embedding layer for multi-hot encoded categorical features.
    Takes multi-hot vectors and produces weighted sum of embeddings.
    """
    
    def __init__(self, n_classes: int, embedding_dim: int, use_counts: bool = False):
        super().__init__()
        self.embedding = nn.Embedding(n_classes, embedding_dim)
        self.n_classes = n_classes
        self.embedding_dim = embedding_dim
        self.use_counts = use_counts
    
    def forward(self, x_multi_hot: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x_multi_hot: [batch_size, seq_len, n_classes]
        
        Returns:
            embedded: [batch_size, seq_len, embedding_dim]
        """
        all_embeddings = self.embedding.weight
        
        if self.use_counts:
            # Normalize counts to probabilities
            count_sum = x_multi_hot.sum(dim=-1, keepdim=True).clamp(min=1.0)
            weights = x_multi_hot / count_sum
            embedded = torch.matmul(weights, all_embeddings)
        else:
            # Simple weighted sum for binary multi-hot
            embedded = torch.matmul(x_multi_hot, all_embeddings)
        
        return embedded

class TSTabFusionTransformerMultiHot(nn.Module):
    """
    TSTabFusionTransformer with multi-hot categorical time series support.
    
    FIXED to work with TSAI's get_mixed_dls format.
    
    Key differences from standard version:
    - Accepts multi-hot encoded categorical TS (multiple labels per timestep)
    - Uses MultiHotEmbedding instead of standard Embedding
    - Handles variable number of active categories per timestep
    
    Use this when your categorical features can have multiple values at once,
    e.g., multiple medications, multiple diagnoses, multiple events.
    """
    
    def __init__(
        self,
        c_in: int,                              # Continuous TS channels
        c_out: int,                             # Output classes
        seq_len: int,                           # Sequence length
        classes: Dict,                          # Static categorical features
        cont_names: List[str],                  # Static continuous features
        ts_cat_dims: Optional[Dict[str, int]] = None,  # NEW: {feature_name: n_classes}
        d_model: int = 32,
        n_layers: int = 6,
        n_heads: int = 8,
        d_k: Optional[int] = None,
        d_v: Optional[int] = None,
        d_ff: Optional[int] = None,
        res_attention: bool = True,
        attention_act: str = 'gelu',
        res_dropout: float = 0.,
        fc_mults: tuple = (1, .5),
        fc_dropout: float = 0.,
        fc_act = None,
        fc_skip: bool = False,
        fc_bn: bool = False,
        bn_final: bool = False,
        init: bool = True,
        key_padding_mask: str = 'auto',
        cat_ts_combine: str = 'add',           # 'add' or 'concat'
        use_count_normalization: bool = False  # NEW: Normalize counts
    ):
        """
        Args:
            ts_cat_dims: Dictionary mapping feature names to their dimensions
                        For multi-hot: dimension = number of possible categories
                        Example: {'medication': 50, 'diagnosis': 100}
        """
        super().__init__()
        self.key_padding_mask = key_padding_mask
        self.cat_ts_combine = cat_ts_combine
        self.use_count_normalization = use_count_normalization
        
        # === MULTI-HOT CATEGORICAL TIME SERIES (NEW) ===
        # Initialize this FIRST to determine W_P size
        if ts_cat_dims is not None:
            self.ts_cat_names = list(ts_cat_dims.keys())
            self.n_ts_cat = len(ts_cat_dims)
            
            if cat_ts_combine == 'concat':
                # Split embedding dimension among features
                emb_dim = max(1, d_model // (self.n_ts_cat + 1))
                self.ts_cat_embeds = nn.ModuleList([
                    MultiHotEmbedding(n_classes, emb_dim, use_count_normalization)
                    for n_classes in ts_cat_dims.values()
                ])
                # Continuous gets remaining dimension
                continuous_dim = d_model - (emb_dim * self.n_ts_cat)
            else:  # 'add'
                # Each feature gets full d_model dimensions
                self.ts_cat_embeds = nn.ModuleList([
                    MultiHotEmbedding(n_classes, d_model, use_count_normalization)
                    for n_classes in ts_cat_dims.values()
                ])
                continuous_dim = d_model
            
            self.ts_cat_dims = ts_cat_dims
        else:
            self.n_ts_cat = 0
            self.ts_cat_embeds = None
            self.ts_cat_dims = {}
            continuous_dim = d_model
        
        # === CONTINUOUS TIME SERIES ===
        # Initialize W_P AFTER determining the correct output dimension
        self.W_P = nn.Conv1d(c_in, continuous_dim, 1)
        
        # === STATIC CATEGORICAL FEATURES ===
        n_cat = len(classes)
        n_classes = [len(v) for v in classes.values()]
        self.n_emb = sum(n_classes)
        self.embeds = nn.ModuleList([nn.Embedding(ni, d_model) for ni in n_classes])
        
        # === STATIC CONTINUOUS FEATURES ===
        n_cont = len(cont_names)
        self.n_cont = n_cont
        self.conv = nn.Conv1d(1, d_model, 1)
        if init:
            nn.init.kaiming_normal_(self.conv.weight)
        
        # === TRANSFORMER ===
        self.res_drop = nn.Dropout(res_dropout) if res_dropout else None
        self.pos_enc = nn.Parameter(torch.zeros(1, (n_cat + n_cont + seq_len), d_model))
        self.transformer = _TabFusionEncoder(
            n_cat + n_cont, d_model, n_heads=n_heads, d_k=d_k, d_v=d_v, d_ff=d_ff,
            res_dropout=res_dropout, activation=attention_act,
            res_attention=res_attention, n_layers=n_layers
        )
        
        # === HEAD ===
        mlp_input_size = (d_model * (n_cat + n_cont + seq_len))
        hidden_dimensions = list(map(lambda t: int(mlp_input_size * t), fc_mults))
        all_dimensions = [mlp_input_size, *hidden_dimensions, c_out]
        self.head_nf = mlp_input_size
        self.head = nn.Sequential(
            _Flatten(),
            _MLP(all_dimensions, act=fc_act, skip=fc_skip, bn=fc_bn,
                 dropout=fc_dropout, bn_final=bn_final)
        )
    
    def forward(self, *x):
        """
        FIXED: Forward pass compatible with fastai's get_preds() and training.
        
        Handles both:
        - Packed input: forward((x_ts, x_tab, x_ts_cat)) - from dataloader
        - Unpacked input: forward(x_ts, x_tab, x_ts_cat) - from get_preds()
        
        TSAI's get_mixed_dls outputs batches in this format:
            ((x_ts, x_tab, x_ts_cat), y)
        
        Where:
            x[0] = x_ts: [batch, c_in, seq_len] - continuous time series
            x[1] = x_tab: (x_cat, x_cont) - static tabular features (tuple)
            x[2] = x_ts_cat: [batch, total_cat_dim, seq_len] - categorical time series
        
        Args:
            *x: Variable arguments to handle both packed and unpacked formats
        """
        
        # === HANDLE PACKED vs UNPACKED INPUTS ===
        if len(x) >= 2:
            # Unpacked: forward(x_ts, x_tab, x_ts_cat) or forward(x_ts, x_tab)
            # This happens with get_preds() and some callbacks
            x_input = x
        elif len(x) == 1:
            # Packed: forward((x_ts, x_tab, x_ts_cat))
            # This happens during normal training
            x_input = x[0]
            if not isinstance(x_input, (tuple, list)):
                x_input = (x_input,)
        else:
            raise ValueError(f"Unexpected number of inputs: {len(x)}")
        
        # === PARSE INPUT (FIXED FOR TSAI FORMAT) ===
        if isinstance(x_input, (tuple, list)):
            if len(x_input) == 3:
                # TSAI format: (x_ts, x_tab, x_ts_cat)
                x_ts = x_input[0]                    # Continuous TS
                x_tab = x_input[1]                   # Tabular (tuple)
                x_ts_cat_multi_hot = x_input[2]      # Categorical TS
                
                # Unpack tabular
                if isinstance(x_tab, (tuple, list)) and len(x_tab) == 2:
                    x_cat, x_cont = x_tab
                else:
                    # Fallback: treat entire x_tab as categorical
                    x_cat = x_tab
                    x_cont = torch.tensor([], device=x_ts.device)
            
            elif len(x_input) == 2:
                # Format without categorical TS: (x_ts, x_tab)
                x_ts = x_input[0]
                x_tab = x_input[1]
                x_ts_cat_multi_hot = None
                
                # Unpack tabular
                if isinstance(x_tab, (tuple, list)) and len(x_tab) == 2:
                    x_cat, x_cont = x_tab
                else:
                    x_cat = x_tab
                    x_cont = torch.tensor([], device=x_ts.device)
            else:
                raise ValueError(
                    f"Expected input with 2 or 3 elements, got {len(x_input)}. "
                    f"TSAI format should be (x_ts, x_tab, x_ts_cat) or (x_ts, x_tab)"
                )
        else:
            # Single tensor input (backward compatibility)
            x_ts = x_input
            x_ts_cat_multi_hot = None
            x_cat = torch.tensor([], device=x_input.device)
            x_cont = torch.tensor([], device=x_input.device)
        
        # === HANDLE KEY PADDING MASK ===
        if self.key_padding_mask == "auto":
            x_ts, key_padding_mask = self._key_padding_mask(x_ts)
        else:
            key_padding_mask = None
        
        # === PROCESS CONTINUOUS TIME SERIES ===
        x = self.W_P(x_ts).transpose(1, 2)  # [bs, seq_len, d_model]
        
        # === PROCESS MULTI-HOT CATEGORICAL TIME SERIES ===
        if self.n_ts_cat > 0 and x_ts_cat_multi_hot is not None:
            # x_ts_cat_multi_hot: [bs, total_cat_dim, seq_len] from TSAI
            # Could be TSTensor (TSAI's custom class) or regular tensor
            
            # Convert TSTensor to regular tensor if needed
            if hasattr(x_ts_cat_multi_hot, 'data'):
                # TSTensor has .data attribute
                x_ts_cat_multi_hot = x_ts_cat_multi_hot.data
            
            # Ensure it's a float tensor for embedding
            x_ts_cat_multi_hot = x_ts_cat_multi_hot.float()
            
            # Need to transpose to [bs, seq_len, total_cat_dim] for embedding
            x_ts_cat_multi_hot = x_ts_cat_multi_hot.transpose(1, 2)
            
            x_ts_cat_embedded_list = []
            dim_offset = 0
            
            for embed_layer, (feat_name, n_classes) in zip(
                self.ts_cat_embeds, self.ts_cat_dims.items()
            ):
                # Extract multi-hot vector for this feature
                feat_multi_hot = x_ts_cat_multi_hot[
                    :, :, dim_offset:dim_offset + n_classes
                ]
                
                # Embed
                feat_embedded = embed_layer(feat_multi_hot)
                x_ts_cat_embedded_list.append(feat_embedded)
                
                dim_offset += n_classes
            
            # Combine embeddings
            if self.cat_ts_combine == 'add':
                # Sum all categorical embeddings
                x_ts_cat_sum = torch.stack(x_ts_cat_embedded_list, dim=0).sum(dim=0)
                x = x + x_ts_cat_sum
            else:  # 'concat'
                # Concatenate all categorical embeddings
                x_ts_cat_concat = torch.cat(x_ts_cat_embedded_list, dim=-1)
                x = torch.cat([x, x_ts_cat_concat], dim=-1)
        
        # === PROCESS STATIC CATEGORICAL FEATURES ===
        if self.n_emb != 0 and x_cat.numel() > 0:
            x_cat_list = [e(x_cat[:, i]).unsqueeze(1) for i, e in enumerate(self.embeds)]
            x_cat_embedded = torch.cat(x_cat_list, 1)
            x = torch.cat([x, x_cat_embedded], 1)
        
        # === PROCESS STATIC CONTINUOUS FEATURES ===
        if self.n_cont != 0 and x_cont.numel() > 0:
            x_cont_proj = self.conv(x_cont.unsqueeze(1)).transpose(1, 2)
            x = torch.cat([x, x_cont_proj], 1)
        
        # === TRANSFORMER ===
        x += self.pos_enc
        
        if self.res_drop is not None:
            x = self.res_drop(x)
        
        x = self.transformer(x, key_padding_mask=key_padding_mask)
        
        if key_padding_mask is not None:
            x = x * torch.logical_not(key_padding_mask.unsqueeze(1))
        
        # === HEAD ===
        x = self.head(x)
        return x
    
    def _key_padding_mask(self, x):
        """Handle NaN values in time series"""
        mask = torch.isnan(x)
        x[mask] = 0
        if mask.any():
            try:
                from tsai.data.core import TSMaskTensor
                mask = TSMaskTensor((mask.float().mean(1) == 1).bool())
            except:
                mask = (mask.float().mean(1) == 1).bool()
            return x, mask
        else:
            return x, None

