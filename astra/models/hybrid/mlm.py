import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import yaml
from pathlib import Path
import matplotlib.pyplot as plt
from dataclasses import dataclass, asdict
from collections import defaultdict
from tqdm.auto import tqdm

# ============================================================================
# Configuration Management
# ============================================================================

@dataclass
class MLMConfig: #TODO: inherit from cfg
    """Configuration for MLM pre-training with multi-hot categorical TS"""
    # Masking strategy
    mask_prob_ts: float = 0.15       # Continuous time series
    mask_prob_cat_ts: float = 0.15   # NEW: Multi-hot categorical TS
    mask_prob_cat: float = 0.15      # Static categorical
    mask_prob_cont: float = 0.15     # Static continuous
    replace_prob: float = 0.8
    random_prob: float = 0.1
    
    # Training hyperparameters
    epochs: int = 50
    lr: float = 1e-4
    weight_decay: float = 0.01
    warmup_epochs: int = 5
    
    # Loss weighting
    ts_loss_weight: float = 1.0
    cat_ts_loss_weight: float = 1.0      # NEW: Multi-hot cat TS loss
    cat_loss_weight: float = 1.0         # Static cat loss
    cont_loss_weight: float = 1.0
    contrastive_weight: float = 0.0
    
    # Contrastive learning
    temperature: float = 0.07
    projection_dim: int = 128
    
    # Checkpointing
    save_best: bool = True
    checkpoint_dir: str = "./checkpoints"
    
    # Early stopping
    patience: int = 10
    min_delta: float = 1e-4
    val_frequency: int = 1

# ============================================================================
# Enhanced MLM Model with Contrastive Learning
# ============================================================================

class TSTabFusionMLM(nn.Module):
    """
    MLM wrapper for TSTabFusionTransformerMultiHot.
    
    Handles:
    - Continuous time series masking & reconstruction
    - Multi-hot categorical time series masking & reconstruction (NEW)
    - Static categorical masking & reconstruction
    - Static continuous masking & reconstruction
    - Contrastive learning across all modalities
    """
    
    def __init__(self, backbone, config: MLMConfig):
        super().__init__()
        self.backbone = backbone
        self.config = config
        
        d_model = backbone.W_P.out_channels
        c_in = backbone.W_P.in_channels
        
        # === RECONSTRUCTION HEADS ===
        
        # 1. Continuous time series reconstruction
        self.ts_head = nn.Sequential(
            nn.Linear(d_model, d_model * 2),
            nn.GELU(),
            nn.LayerNorm(d_model * 2),
            nn.Dropout(0.1),
            nn.Linear(d_model * 2, c_in)
        )
        
        # 2. Multi-hot categorical TS reconstruction (NEW)
        if backbone.n_ts_cat > 0:
            self.cat_ts_heads = nn.ModuleDict()
            for feat_name, n_classes in backbone.ts_cat_dims.items():
                # Reconstruction head outputs logits for each class
                # Multi-label: use sigmoid activation (not softmax)
                self.cat_ts_heads[feat_name] = nn.Sequential(
                    nn.Linear(d_model, d_model),
                    nn.GELU(),
                    nn.Dropout(0.1),
                    nn.Linear(d_model, n_classes)
                )
        else:
            self.cat_ts_heads = None
        
        # 3. Static categorical reconstruction
        if backbone.n_emb != 0:
            n_classes = [emb.num_embeddings for emb in backbone.embeds]
            self.cat_heads = nn.ModuleList([
                nn.Sequential(
                    nn.Linear(d_model, d_model),
                    nn.GELU(),
                    nn.Dropout(0.1),
                    nn.Linear(d_model, n_class)
                ) for n_class in n_classes
            ])
        else:
            self.cat_heads = None
        
        # 4. Static continuous reconstruction
        if backbone.n_cont != 0:
            self.cont_head = nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU(),
                nn.Dropout(0.1),
                nn.Linear(d_model, 1)
            )
        else:
            self.cont_head = None
        
        # 5. Contrastive learning projection
        if config.contrastive_weight > 0:
            self.projection_head = nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU(),
                nn.Linear(d_model, config.projection_dim)
            )
        else:
            self.projection_head = None
    
    def create_mlm_mask(self, shape, device, mask_prob):
        """Create random mask for MLM."""
        return torch.rand(shape, device=device) < mask_prob
    
    def mask_time_series(self, x_ts):
        """Mask continuous time series data."""
        bs, c_in, seq_len = x_ts.shape
        device = x_ts.device
        
        # Create mask per timestep (same mask for all channels)
        mask = self.create_mlm_mask((bs, seq_len), device, self.config.mask_prob_ts)
        
        original_x_ts = x_ts.clone()
        masked_x_ts = x_ts.clone()
        
        for i in range(bs):
            for t in range(seq_len):
                if mask[i, t]:
                    rand_val = torch.rand(1).item()
                    if rand_val < self.config.replace_prob:
                        # Replace with zeros
                        masked_x_ts[i, :, t] = 0
                    elif rand_val < self.config.replace_prob + self.config.random_prob:
                        # Replace with random timestep
                        random_t = torch.randint(0, seq_len, (1,)).item()
                        masked_x_ts[i, :, t] = x_ts[i, :, random_t]
        
        return masked_x_ts, mask, original_x_ts
    
    def mask_categorical_ts(self, x_ts_cat):
        """
        Mask multi-hot categorical time series (NEW).
        
        Args:
            x_ts_cat: [bs, n_categories, seq_len] - multi-hot encoded
        
        Returns:
            masked, mask, original
        """
        if x_ts_cat is None:
            return None, None, None
        
        # Convert TSTensor to regular tensor if needed
        if hasattr(x_ts_cat, 'data'):
            x_ts_cat = x_ts_cat.data
        
        x_ts_cat = x_ts_cat.float()
        
        bs, n_categories, seq_len = x_ts_cat.shape
        device = x_ts_cat.device
        
        # Create mask per timestep (mask all categories at once)
        mask = self.create_mlm_mask((bs, seq_len), device, self.config.mask_prob_cat_ts)
        
        original_x_ts_cat = x_ts_cat.clone()
        masked_x_ts_cat = x_ts_cat.clone()
        
        for i in range(bs):
            for t in range(seq_len):
                if mask[i, t]:
                    rand_val = torch.rand(1).item()
                    if rand_val < self.config.replace_prob:
                        # Replace with all zeros (no categories active)
                        masked_x_ts_cat[i, :, t] = 0
                    elif rand_val < self.config.replace_prob + self.config.random_prob:
                        # Replace with random timestep
                        random_t = torch.randint(0, seq_len, (1,)).item()
                        masked_x_ts_cat[i, :, t] = x_ts_cat[i, :, random_t]
        
        return masked_x_ts_cat, mask, original_x_ts_cat
    
    def mask_categorical(self, x_cat):
        """Mask static categorical features."""
        if x_cat is None or x_cat.numel() == 0:
            return x_cat, None, x_cat
        
        bs, n_cat = x_cat.shape
        device = x_cat.device
        
        mask = self.create_mlm_mask((bs, n_cat), device, self.config.mask_prob_cat)
        
        original_x_cat = x_cat.clone()
        masked_x_cat = x_cat.clone()
        
        for i in range(bs):
            for j in range(n_cat):
                if mask[i, j]:
                    rand_val = torch.rand(1).item()
                    n_classes = self.backbone.embeds[j].num_embeddings
                    if rand_val < self.config.replace_prob:
                        masked_x_cat[i, j] = 0
                    elif rand_val < self.config.replace_prob + self.config.random_prob:
                        masked_x_cat[i, j] = torch.randint(0, n_classes, (1,)).item()
        
        return masked_x_cat, mask, original_x_cat
    
    def mask_continuous(self, x_cont):
        """Mask static continuous features."""
        if x_cont is None or x_cont.numel() == 0:
            return x_cont, None, x_cont
        
        bs, n_cont = x_cont.shape
        device = x_cont.device
        
        mask = self.create_mlm_mask((bs, n_cont), device, self.config.mask_prob_cont)
        
        original_x_cont = x_cont.clone()
        masked_x_cont = x_cont.clone()
        
        # Use feature means for better masking
        feature_means = x_cont.mean(dim=0, keepdim=True)
        
        for i in range(bs):
            for j in range(n_cont):
                if mask[i, j]:
                    rand_val = torch.rand(1).item()
                    if rand_val < self.config.replace_prob:
                        masked_x_cont[i, j] = feature_means[0, j]
                    elif rand_val < self.config.replace_prob + self.config.random_prob:
                        random_idx = torch.randint(0, bs, (1,)).item()
                        masked_x_cont[i, j] = x_cont[random_idx, j]
        
        return masked_x_cont, mask, original_x_cont
    
    def forward_encoder(self, x_ts, x_ts_cat, x_cat, x_cont):
        """
        Forward pass through encoder (UPDATED for TSAI format).
        
        Args:
            x_ts: Continuous TS
            x_ts_cat: Multi-hot categorical TS (NEW)
            x_cat: Static categorical
            x_cont: Static continuous
        """
        # Pack into TSAI format: (x_ts, x_tab, x_ts_cat)
        x_tab = (x_cat, x_cont)
        x = (x_ts, x_tab, x_ts_cat)
        
        # Use backbone's forward (handles everything internally)
        # Get intermediate representations before the head
        # We need to access transformer output, not final classification
        
        # Handle NaN
        if self.backbone.key_padding_mask == "auto":
            x_ts, key_padding_mask = self.backbone._key_padding_mask(x_ts)
        else:
            key_padding_mask = None
        
        # Continuous TS encoding
        x_encoded = self.backbone.W_P(x_ts).transpose(1, 2)  # [bs, seq_len, d_model]
        
        # Multi-hot categorical TS encoding (if present)
        if self.backbone.n_ts_cat > 0 and x_ts_cat is not None:
            # Convert TSTensor if needed
            if hasattr(x_ts_cat, 'data'):
                x_ts_cat = x_ts_cat.data
            x_ts_cat = x_ts_cat.float()
            
            # Transpose to [bs, seq_len, n_categories]
            x_ts_cat = x_ts_cat.transpose(1, 2)
            
            x_ts_cat_embedded_list = []
            dim_offset = 0
            
            for embed_layer, (feat_name, n_classes) in zip(
                self.backbone.ts_cat_embeds, self.backbone.ts_cat_dims.items()
            ):
                feat_multi_hot = x_ts_cat[:, :, dim_offset:dim_offset + n_classes]
                feat_embedded = embed_layer(feat_multi_hot)
                x_ts_cat_embedded_list.append(feat_embedded)
                dim_offset += n_classes
            
            # Combine embeddings
            if self.backbone.cat_ts_combine == 'add':
                x_ts_cat_sum = torch.stack(x_ts_cat_embedded_list, dim=0).sum(dim=0)
                x_encoded = x_encoded + x_ts_cat_sum
            else:  # 'concat'
                x_ts_cat_concat = torch.cat(x_ts_cat_embedded_list, dim=-1)
                x_encoded = torch.cat([x_encoded, x_ts_cat_concat], dim=-1)
        
        # Static categorical encoding
        if self.backbone.n_emb != 0 and x_cat is not None and x_cat.numel() > 0:
            x_cat_list = [e(x_cat[:, i]).unsqueeze(1) for i, e in enumerate(self.backbone.embeds)]
            x_cat_embedded = torch.cat(x_cat_list, 1)
            x_encoded = torch.cat([x_encoded, x_cat_embedded], 1)
        
        # Static continuous encoding
        if self.backbone.n_cont != 0 and x_cont is not None and x_cont.numel() > 0:
            x_cont_proj = self.backbone.conv(x_cont.unsqueeze(1)).transpose(1, 2)
            x_encoded = torch.cat([x_encoded, x_cont_proj], 1)
        
        # Positional encoding
        x_encoded += self.backbone.pos_enc
        
        if self.backbone.res_drop is not None:
            x_encoded = self.backbone.res_drop(x_encoded)
        
        # Transformer
        x_encoded = self.backbone.transformer(x_encoded, key_padding_mask=key_padding_mask)
        
        if key_padding_mask is not None:
            x_encoded = x_encoded * torch.logical_not(key_padding_mask.unsqueeze(1))
        
        return x_encoded
    
    def contrastive_loss(self, z1, z2):
        """Compute NT-Xent contrastive loss."""
        batch_size = z1.shape[0]
        
        z1 = F.normalize(z1, dim=1)
        z2 = F.normalize(z2, dim=1)
        
        representations = torch.cat([z1, z2], dim=0)
        similarity_matrix = F.cosine_similarity(
            representations.unsqueeze(1), 
            representations.unsqueeze(0), 
            dim=2
        )
        
        labels = torch.arange(batch_size, device=z1.device)
        labels = torch.cat([labels + batch_size, labels], dim=0)
        
        mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z1.device)
        similarity_matrix = similarity_matrix.masked_fill(mask, -float('inf'))
        
        similarity_matrix = similarity_matrix / self.config.temperature
        loss = F.cross_entropy(similarity_matrix, labels)
        
        return loss
    
    def forward(self, x_ts, x_ts_cat, x_cat, x_cont, return_contrastive=False):
        """
        Forward pass with MLM (UPDATED for multi-hot categorical TS).
        
        Args:
            x_ts: [bs, c_in, seq_len] - continuous time series
            x_ts_cat: [bs, n_categories, seq_len] - multi-hot categorical TS
            x_cat: [bs, n_cat] - static categorical
            x_cont: [bs, n_cont] - static continuous
            return_contrastive: Whether to compute contrastive loss
        """
        # Create augmented views for contrastive learning
        if return_contrastive and self.config.contrastive_weight > 0:
            # First view
            masked_ts1, ts_mask1, original_ts = self.mask_time_series(x_ts)
            masked_ts_cat1, ts_cat_mask1, original_ts_cat = self.mask_categorical_ts(x_ts_cat)
            masked_cat1, cat_mask1, original_cat = self.mask_categorical(x_cat)
            masked_cont1, cont_mask1, original_cont = self.mask_continuous(x_cont)
            
            # Second view
            masked_ts2, _, _ = self.mask_time_series(x_ts)
            masked_ts_cat2, _, _ = self.mask_categorical_ts(x_ts_cat)
            masked_cat2, _, _ = self.mask_categorical(x_cat)
            masked_cont2, _, _ = self.mask_continuous(x_cont)
            
            encoder_output1 = self.forward_encoder(masked_ts1, masked_ts_cat1, masked_cat1, masked_cont1)
            encoder_output2 = self.forward_encoder(masked_ts2, masked_ts_cat2, masked_cat2, masked_cont2)
            
            encoder_output = encoder_output1
            ts_mask, ts_cat_mask = ts_mask1, ts_cat_mask1
            cat_mask, cont_mask = cat_mask1, cont_mask1
            
            # Global pooling for contrastive
            z1 = encoder_output1.mean(dim=1)
            z2 = encoder_output2.mean(dim=1)
            z1 = self.projection_head(z1)
            z2 = self.projection_head(z2)
            
            contrastive_loss = self.contrastive_loss(z1, z2)
        else:
            masked_ts, ts_mask, original_ts = self.mask_time_series(x_ts)
            masked_ts_cat, ts_cat_mask, original_ts_cat = self.mask_categorical_ts(x_ts_cat)
            masked_cat, cat_mask, original_cat = self.mask_categorical(x_cat)
            masked_cont, cont_mask, original_cont = self.mask_continuous(x_cont)
            
            encoder_output = self.forward_encoder(masked_ts, masked_ts_cat, masked_cat, masked_cont)
            contrastive_loss = None
        
        seq_len = x_ts.shape[2]
        n_cat = x_cat.shape[1] if x_cat is not None and x_cat.numel() > 0 else 0
        n_cont = x_cont.shape[1] if x_cont is not None and x_cont.numel() > 0 else 0
        
        # Split encoder output
        ts_output = encoder_output[:, :seq_len, :]
        cat_output = encoder_output[:, seq_len:seq_len+n_cat, :] if n_cat > 0 else None
        cont_output = encoder_output[:, seq_len+n_cat:, :] if n_cont > 0 else None
        
        losses = {}
        
        # 1. Continuous TS reconstruction
        if ts_mask is not None and ts_mask.any():
            ts_pred = self.ts_head(ts_output).transpose(1, 2)  # [bs, c_in, seq_len]
            ts_loss = F.mse_loss(
                ts_pred[ts_mask.unsqueeze(1).expand_as(ts_pred)],
                original_ts[ts_mask.unsqueeze(1).expand_as(original_ts)]
            )
            losses['ts_loss'] = ts_loss * self.config.ts_loss_weight
        
        # 2. Multi-hot categorical TS reconstruction (NEW)
        if ts_cat_mask is not None and ts_cat_mask.any() and self.cat_ts_heads is not None:
            cat_ts_losses = []
            dim_offset = 0
            
            for feat_name, n_classes in self.backbone.ts_cat_dims.items():
                # Get predictions for this feature
                feat_pred = self.cat_ts_heads[feat_name](ts_output)  # [bs, seq_len, n_classes]
                feat_pred = feat_pred.transpose(1, 2)  # [bs, n_classes, seq_len]
                
                # Get target multi-hot vectors
                feat_target = original_ts_cat[:, dim_offset:dim_offset+n_classes, :]
                
                # Multi-label BCE loss (not cross-entropy!)
                feat_loss = F.binary_cross_entropy_with_logits(
                    feat_pred[ts_cat_mask.unsqueeze(1).expand_as(feat_pred)],
                    feat_target[ts_cat_mask.unsqueeze(1).expand_as(feat_target)]
                )
                cat_ts_losses.append(feat_loss)
                dim_offset += n_classes
            
            if cat_ts_losses:
                losses['cat_ts_loss'] = torch.stack(cat_ts_losses).mean() * self.config.cat_ts_loss_weight
        
        # 3. Static categorical reconstruction
        if cat_mask is not None and cat_mask.any() and cat_output is not None:
            cat_losses = []
            for i in range(n_cat):
                if cat_mask[:, i].any():
                    cat_pred = self.cat_heads[i](cat_output[:, i, :])
                    cat_loss = F.cross_entropy(
                        cat_pred[cat_mask[:, i]],
                        original_cat[cat_mask[:, i], i]
                    )
                    cat_losses.append(cat_loss)
            if cat_losses:
                losses['cat_loss'] = torch.stack(cat_losses).mean() * self.config.cat_loss_weight
        
        # 4. Static continuous reconstruction
        if cont_mask is not None and cont_mask.any() and cont_output is not None:
            cont_pred = self.cont_head(cont_output).squeeze(-1)
            cont_loss = F.mse_loss(
                cont_pred[cont_mask],
                original_cont[cont_mask]
            )
            losses['cont_loss'] = cont_loss * self.config.cont_loss_weight
        
        # 5. Contrastive loss
        if contrastive_loss is not None:
            losses['contrastive_loss'] = contrastive_loss * self.config.contrastive_weight
        
        # Total loss
        total_loss = sum(losses.values()) if losses else torch.tensor(0.0, device=x_ts.device)
        losses['total_loss'] = total_loss
        
        return losses


# ============================================================================
# Training with All Enhancements
# ============================================================================

class EarlyStopping:
    """Early stopping helper."""
    def __init__(self, patience=10, min_delta=1e-4, mode='min'):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        
    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
        elif self.mode == 'min':
            if score > self.best_score - self.min_delta:
                self.counter += 1
                if self.counter >= self.patience:
                    self.early_stop = True
            else:
                self.best_score = score
                self.counter = 0
        
        return self.early_stop


def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):
    """Create learning rate scheduler with warmup."""
    def lr_lambda(current_step):
        if current_step < num_warmup_steps:
            return float(current_step) / float(max(1, num_warmup_steps))
        progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
        return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))
    
    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)


def pretrain_mlm_enhanced(
    model,
    train_loader,
    config,
    val_loader=None,
    device='cuda'
):
    """
    Enhanced pre-training for multi-hot categorical TS.
    
    UPDATED to handle TSAI's get_mixed_dls format:
        Batch format: ((x_ts, x_tab, x_ts_cat), y)
        where x_tab = (x_cat, x_cont)
    
    Args:
        model: TSTabFusionMLM instance
        train_loader: TSAI DataLoader from get_mixed_dls
        config: MLMConfig
        val_loader: Optional validation DataLoader
        device: Device to use
    
    Returns:
        history: Training history dict
    """
    model = model.to(device)
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.lr,
        weight_decay=config.weight_decay
    )
    
    # Learning rate scheduler
    num_training_steps = config.epochs * len(train_loader)
    num_warmup_steps = config.warmup_epochs * len(train_loader)
    scheduler = get_cosine_schedule_with_warmup(
        optimizer, num_warmup_steps, num_training_steps
    )
    
    # Early stopping
    early_stopping = EarlyStopping(
        patience=config.patience,
        min_delta=config.min_delta
    )
    
    # Checkpoint directory
    checkpoint_dir = Path(config.checkpoint_dir)
    checkpoint_dir.mkdir(exist_ok=True, parents=True)
    
    # Training history
    history = {
        'train_loss': [],
        'val_loss': [],
        'ts_loss': [],
        'cat_ts_loss': [],      # NEW
        'cat_loss': [],
        'cont_loss': [],
        'contrastive_loss': [],
        'lr': []
    }
    
    best_val_loss = float('inf')
    
    for epoch in range(config.epochs):
        # === TRAINING ===
        model.train()
        meters = defaultdict(lambda: {'sum': 0, 'count': 0})
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{config.epochs}")
        
        for batch in pbar:
            # Unpack TSAI format: ((x_ts, x_tab, x_ts_cat), y)
            inputs, targets = batch
            
            # inputs is a tuple: (x_ts, x_tab, x_ts_cat)
            x_ts = inputs[0]           # Continuous TS
            x_tab = inputs[1]          # Tabular (tuple)
            x_ts_cat = inputs[2]       # Categorical TS (multi-hot)
            
            # Unpack tabular
            x_cat, x_cont = x_tab
            
            # Move to device
            x_ts = x_ts.to(device)
            x_ts_cat = x_ts_cat.to(device) if x_ts_cat is not None else None
            x_cat = x_cat.to(device) if x_cat is not None and x_cat.numel() > 0 else None
            x_cont = x_cont.to(device) if x_cont is not None and x_cont.numel() > 0 else None
            
            # Forward pass
            optimizer.zero_grad()
            
            losses = model(
                x_ts, x_ts_cat, x_cat, x_cont,
                return_contrastive=(config.contrastive_weight > 0)
            )
            
            loss = losses['total_loss']
            
            # Backward pass
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            scheduler.step()
            
            # Update meters
            meters['total']['sum'] += loss.item()
            meters['total']['count'] += 1
            
            for loss_name in ['ts_loss', 'cat_ts_loss', 'cat_loss', 'cont_loss', 'contrastive_loss']:
                if loss_name in losses:
                    meters[loss_name]['sum'] += losses[loss_name].item()
                    meters[loss_name]['count'] += 1
            
            # Update progress bar
            postfix = {
                'Loss': f"{meters['total']['sum']/meters['total']['count']:.4f}",
                'LR': f"{scheduler.get_last_lr()[0]:.2e}"
            }
            
            if meters['ts_loss']['count'] > 0:
                postfix['TS'] = f"{meters['ts_loss']['sum']/meters['ts_loss']['count']:.4f}"
            if meters['cat_ts_loss']['count'] > 0:
                postfix['CatTS'] = f"{meters['cat_ts_loss']['sum']/meters['cat_ts_loss']['count']:.4f}"
            if meters['cat_loss']['count'] > 0:
                postfix['Cat'] = f"{meters['cat_loss']['sum']/meters['cat_loss']['count']:.4f}"
            if meters['cont_loss']['count'] > 0:
                postfix['Cont'] = f"{meters['cont_loss']['sum']/meters['cont_loss']['count']:.4f}"
            if meters['contrastive_loss']['count'] > 0:
                postfix['Contr'] = f"{meters['contrastive_loss']['sum']/meters['contrastive_loss']['count']:.4f}"
            
            pbar.set_postfix(postfix)
        
        # Record training metrics
        avg_train_loss = meters['total']['sum'] / meters['total']['count']
        history['train_loss'].append(avg_train_loss)
        history['ts_loss'].append(
            meters['ts_loss']['sum'] / meters['ts_loss']['count'] if meters['ts_loss']['count'] > 0 else 0
        )
        history['cat_ts_loss'].append(
            meters['cat_ts_loss']['sum'] / meters['cat_ts_loss']['count'] if meters['cat_ts_loss']['count'] > 0 else 0
        )
        history['cat_loss'].append(
            meters['cat_loss']['sum'] / meters['cat_loss']['count'] if meters['cat_loss']['count'] > 0 else 0
        )
        history['cont_loss'].append(
            meters['cont_loss']['sum'] / meters['cont_loss']['count'] if meters['cont_loss']['count'] > 0 else 0
        )
        history['contrastive_loss'].append(
            meters['contrastive_loss']['sum'] / meters['contrastive_loss']['count'] if meters['contrastive_loss']['count'] > 0 else 0
        )
        history['lr'].append(scheduler.get_last_lr()[0])
        
        # === VALIDATION ===
        val_loss = None
        if val_loader is not None and (epoch + 1) % config.val_frequency == 0:
            model.eval()
            val_meters = {'total': {'sum': 0, 'count': 0}}
            
            with torch.no_grad():
                for batch in tqdm(val_loader, desc="Validating", leave=False):
                    # Unpack TSAI format
                    inputs, targets = batch
                    x_ts = inputs[0]
                    x_tab = inputs[1]
                    x_ts_cat = inputs[2]
                    x_cat, x_cont = x_tab
                    
                    # Move to device
                    x_ts = x_ts.to(device)
                    x_ts_cat = x_ts_cat.to(device) if x_ts_cat is not None else None
                    x_cat = x_cat.to(device) if x_cat is not None and x_cat.numel() > 0 else None
                    x_cont = x_cont.to(device) if x_cont is not None and x_cont.numel() > 0 else None
                    
                    losses = model(x_ts, x_ts_cat, x_cat, x_cont)
                    
                    val_meters['total']['sum'] += losses['total_loss'].item()
                    val_meters['total']['count'] += 1
            
            val_loss = val_meters['total']['sum'] / val_meters['total']['count']
            history['val_loss'].append(val_loss)
            
            # Save best model
            if config.save_best and val_loss < best_val_loss:
                best_val_loss = val_loss
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'val_loss': val_loss,
                    'config': config
                }, checkpoint_dir / 'best_model.pt')
                print(f"  âœ“ Saved best model (val_loss: {val_loss:.4f})")
            
            # Early stopping
            if early_stopping(val_loss):
                print(f"\nEarly stopping triggered at epoch {epoch+1}")
                break
        
        # Print epoch summary
        print(f"Epoch {epoch+1}/{config.epochs} - Train Loss: {avg_train_loss:.4f}")
        if val_loss is not None:
            print(f"  Val Loss: {val_loss:.4f}")
    
    # Plot training curves
    plot_training_curves(history, save_path=checkpoint_dir / 'training_curves.png')
    
    return history


def plot_training_curves(history, save_path='training_curves.png'):
    """Plot training curves with multi-hot categorical TS loss."""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. Total loss
    axes[0, 0].plot(history['train_loss'], label='Train Loss', linewidth=2)
    if history['val_loss']:
        val_epochs = np.linspace(0, len(history['train_loss'])-1, len(history['val_loss']))
        axes[0, 0].plot(val_epochs, history['val_loss'], label='Val Loss', linewidth=2, linestyle='--')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Total Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Component losses
    if any(history['ts_loss']):
        axes[0, 1].plot(history['ts_loss'], label='TS Loss')
    if any(history['cat_ts_loss']):
        axes[0, 1].plot(history['cat_ts_loss'], label='Cat TS Loss', linewidth=2)  # NEW
    if any(history['cat_loss']):
        axes[0, 1].plot(history['cat_loss'], label='Static Cat Loss')
    if any(history['cont_loss']):
        axes[0, 1].plot(history['cont_loss'], label='Static Cont Loss')
    if any(history['contrastive_loss']):
        axes[0, 1].plot(history['contrastive_loss'], label='Contrastive Loss')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('Loss')
    axes[0, 1].set_title('Component Losses')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. Learning rate
    axes[1, 0].plot(history['lr'], linewidth=2)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Learning Rate')
    axes[1, 0].set_title('Learning Rate Schedule')
    axes[1, 0].set_yscale('log')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. Loss ratios
    if any(history['ts_loss']) and any(history['cat_ts_loss']):
        ts_cat_ts_ratio = np.array(history['ts_loss']) / (np.array(history['cat_ts_loss']) + 1e-8)
        axes[1, 1].plot(ts_cat_ts_ratio, label='TS/CatTS Ratio')
    if any(history['cat_ts_loss']) and any(history['cat_loss']):
        cat_ts_cat_ratio = np.array(history['cat_ts_loss']) / (np.array(history['cat_loss']) + 1e-8)
        axes[1, 1].plot(cat_ts_cat_ratio, label='CatTS/StaticCat Ratio')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Loss Ratio')
    axes[1, 1].set_title('Loss Component Ratios')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.show()

